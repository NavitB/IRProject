{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Search.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhblBUmVkw68WONfWjOM59",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NavitB/IRProject/blob/main/Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search"
      ],
      "metadata": {
        "id": "X9Kx8A5FMkBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate posting lists"
      ],
      "metadata": {
        "id": "wYWv_9mONFWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_posting_gen(index):\n",
        "    \"\"\"\n",
        "    This function returning the generator working with posting list.\n",
        "    \n",
        "    Parameters:\n",
        "    ----------\n",
        "    index: inverted index    \n",
        "    \"\"\"\n",
        "    words, pls = zip(*index.posting_lists_iter())\n",
        "    return words,pls"
      ],
      "metadata": {
        "id": "KEDkO43tNKOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_text, pls_text = get_posting_gen(inverted_text)\n",
        "words_title, pls_title = get_posting_gen(inverted_title)"
      ],
      "metadata": {
        "id": "IGcOJp_INPB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search Body"
      ],
      "metadata": {
        "id": "sN6M8CidMsnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_query_tfidf_vector(query_to_search,index):\n",
        "    \"\"\" \n",
        "    Generate a vector representing the query. Each entry within this vector represents a tfidf score.\n",
        "    The terms representing the query will be the unique terms in the index.\n",
        "\n",
        "    We will use tfidf on the query as well. \n",
        "    For calculation of IDF, use log with base 10.\n",
        "    tf will be normalized based on the length of the query.    \n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    vectorized query with tfidf scores\n",
        "    \"\"\"\n",
        "    \n",
        "    epsilon = .0000001\n",
        "    total_vocab_size = len(index.term_total)\n",
        "    Q = np.zeros((total_vocab_size))\n",
        "    term_vector = list(index.term_total.keys()) \n",
        "    counter = Counter(query_to_search)\n",
        "    for token in np.unique(query_to_search):\n",
        "        if token in index.term_total.keys(): #avoid terms that do not appear in the index.               \n",
        "            tf = counter[token]/len(query_to_search) # term frequency divded by the length of the query\n",
        "            df = index.df[token]            \n",
        "            idf = math.log((len(index.DL))/(df+epsilon),10) #smoothing\n",
        "            try:\n",
        "                ind = term_vector.index(token)\n",
        "                Q[ind] = tf*idf                    \n",
        "            except:\n",
        "                pass\n",
        "    return Q\n",
        "    "
      ],
      "metadata": {
        "id": "pOhUzdskMrcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def get_candidate_documents_and_scores(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search\n",
        "    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.\n",
        "    Then it will populate the dictionary 'candidates.'\n",
        "    For calculation of IDF, use log with base 10.\n",
        "    tf will be normalized based on the length of the document.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    words,pls: generator for working with posting.\n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of candidates. In the following format:\n",
        "                                                               key: pair (doc_id,term)\n",
        "                                                               value: tfidf score. \n",
        "    \"\"\"\n",
        "    candidates = {}\n",
        "    N = len(index.DL)\n",
        "    for term in np.unique(query_to_search):  \n",
        "        if term in words:            \n",
        "            list_of_doc = pls[words.index(term)]\n",
        "            normlized_tfidf = [(doc_id,(freq/index.DL[doc_id])*math.log(N/index.df[term],10)) for doc_id, freq in list_of_doc]           \n",
        "            for doc_id, tfidf in normlized_tfidf:\n",
        "                candidates[(doc_id,term)] = candidates.get((doc_id,term), 0) + tfidf               \n",
        "    \n",
        "    return candidates\n",
        "\n"
      ],
      "metadata": {
        "id": "jW-SGQktM0QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_document_tfidf_matrix(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a DataFrame `D` of tfidf scores for a given query. \n",
        "    Rows will be the documents candidates for a given query\n",
        "    Columns will be the unique terms in the index.\n",
        "    The value for a given document and term will be its tfidf score.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    words,pls: generator for working with posting.\n",
        "    Returns:\n",
        "    -----------\n",
        "    DataFrame of tfidf scores.\n",
        "    \"\"\"\n",
        "    \n",
        "    total_vocab_size = len(index.term_total)\n",
        "    candidates_scores = get_candidate_documents_and_scores(query_to_search,index,words,pls) #We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.\n",
        "    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])\n",
        "    D = np.zeros((len(unique_candidates), total_vocab_size))\n",
        "    D = pd.DataFrame(D)\n",
        "    \n",
        "    D.index = unique_candidates\n",
        "    D.columns = index.term_total.keys()\n",
        "\n",
        "    for key in candidates_scores:\n",
        "        tfidf = candidates_scores[key]\n",
        "        doc_id, term = key    \n",
        "        D.loc[doc_id][term] = tfidf\n",
        "\n",
        "\n",
        "    return D\n",
        "     "
      ],
      "metadata": {
        "id": "KXfbyXdKM4Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(D,Q,index):\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).\n",
        "    Generate a dictionary of cosine similarity scores \n",
        "    key: doc_id\n",
        "    value: cosine similarity score\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    D: DataFrame of tfidf scores.\n",
        "\n",
        "    Q: vectorized query with tfidf scores\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of cosine similarity score as follows:\n",
        "                                                                key: document id (e.g., doc_id)\n",
        "                                                                value: cosine similarty score.\n",
        "    \"\"\"\n",
        "    dict_cos = {}\n",
        "    for doc_id, row in D.iterrows(): \n",
        "      vec_doc = row.to_numpy()\n",
        "      result = np.dot(vec_doc, Q)/(index.vec_len_doc[doc_id] * np.linalg.norm(Q))\n",
        "      dict_cos[doc_id] = result\n",
        "    return dict_cos\n"
      ],
      "metadata": {
        "id": "VC6pnH0mM6pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n(sim_dict,N=3):\n",
        "    \"\"\" \n",
        "    Sort and return the highest N documents according to the cosine similarity score.\n",
        "    Generate a dictionary of cosine similarity scores \n",
        "   \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sim_dict: a dictionary of similarity score as follows:\n",
        "                                                                key: document id (e.g., doc_id)\n",
        "                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))\n",
        "\n",
        "    N: Integer (how many documents to retrieve). By default N = 3\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    a ranked list of pairs (doc_id, score) in the length of N.\n",
        "    \"\"\"\n",
        "    lst = [(doc_id , builtins.round(score,5)) for doc_id, score in sim_dict.items()]\n",
        "    return sorted(lst, key = lambda x: x[1], reverse=True)[:N]\n",
        "    "
      ],
      "metadata": {
        "id": "CHxgOBBkM9Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topN_score_for_queries(queries_to_search,index, words, pls,N=3):\n",
        "    \"\"\" \n",
        "    Generate a dictionary that gathers for every query its topN score.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    queries_to_search: a dictionary of queries as follows: \n",
        "                                                        key: query_id\n",
        "                                                        value: list of tokens.\n",
        "    index:           inverted index loaded from the corresponding files.    \n",
        "    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function. \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    return: a dictionary of queries and topN pairs as follows:\n",
        "                                                        key: query_id\n",
        "                                                        value: list of pairs in the following format:(doc_id, score). \n",
        "    \"\"\"\n",
        "    top_N = {}\n",
        "    for q_id, q in queries_to_search.items() :\n",
        "      Q = generate_query_tfidf_vector(q,index)\n",
        "      D = generate_document_tfidf_matrix(q, index, words, pls)\n",
        "      top_N[q_id] = get_top_n(cosine_similarity(D, Q, index), N)\n",
        "    return top_N\n"
      ],
      "metadata": {
        "id": "yxSy94TFM_0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "efd-pqbvNB-7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}